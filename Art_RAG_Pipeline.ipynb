{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b590b-ccfb-475b-9ab2-c152a66ea555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This requires two dependencies: langchain and langchain_aws\n",
    "# You also need datasets to use the wikiart-subject dataset, and datasets requires Pillow to decode images\n",
    "# I've also chosen to use FAISS as a vector store & cache, which requires faiss-cpu and langchain-community\n",
    "# pip install langchain langchain_aws\n",
    "# pip install datasets Pillow\n",
    "# pip install faiss-cpu langchain-community\n",
    "\n",
    "# Available models:\n",
    "# amazon.titan-embed-text-v1\n",
    "# amazon.titan-embed-image-v1\n",
    "# anthropic.claude-3-5-sonnet-20240620-v1:0\n",
    "# cohere.embed-multilingual-v3\n",
    "# meta.llama3-70b-instruct-v1:0\n",
    "\n",
    "import boto3\n",
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain_aws.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "from datasets import load_dataset\n",
    "import faiss\n",
    "import os\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "# Load the dataset:\n",
    "# https://huggingface.co/datasets/jlbaker361/wikiart-subjects\n",
    "full_dataset = load_dataset(\"jlbaker361/wikiart-subjects\")\n",
    "\n",
    "# For development, let's use a smaller subset of the full dataset, since it's quite large (815MB)\n",
    "# Let's take a 5% random sample from the \"train\" split.\n",
    "#small_dataset = full_dataset[\"train\"].train_test_split(test_size=0.05)[\"test\"]\n",
    "\n",
    "# Optionally, if we set also a seed, we'll get the same subset each time; the consistency can be handy for testing & debugging.\n",
    "small_dataset = full_dataset[\"train\"].train_test_split(test_size=0.05, seed=42)[\"test\"]\n",
    "\n",
    "# Just to see that the data is there, convert it to DataFrame and display the first few rows\n",
    "small_dataset_df = small_dataset.to_pandas()\n",
    "print(small_dataset_df.head())  # Display the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d302793-f5f4-4f13-9d67-e0a43525179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a session with AWS via the Boto3 Python SDK.\n",
    "session = boto3.Session(\n",
    "  aws_access_key_id='[AWS_ACCESS_KEY_ID]',\n",
    "  aws_secret_access_key='[AWS_SECRET_ACCESS_KEY]',\n",
    "  region_name='us-east-1'\n",
    ")\n",
    "\n",
    "# Connect to Bedrock services so we can access models for embeddings & chat.\n",
    "client = session.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f4a9dc-2cb8-463e-b549-93f558f6df5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to create embeddings for our dataset and store them in a vector database.\n",
    "# I have chosen FAISS as a vector db because it runs locally instead of in the cloud,\n",
    "# so I don't have to sign up for an account, a subscription, etc.\n",
    "# \n",
    "# For added efficiency, I'm also storing the FAISS db in a local index file, to act as a cache\n",
    "# so that I don't recreate embeds over and over if I restart Jupyter, preventing wasteful API calls to AWS.\n",
    "\n",
    "text_embed_model = 'amazon.titan-embed-text-v1'\n",
    "image_embed_model = 'amazon.titan-embed-image-v1'\n",
    "INDEX_PATH = \"faiss_index_file\"  # File path for saving/loading FAISS index\n",
    "dimension = 1024  # Default vector dimensions for Amazon's titan embed models; 384 and 256 also supported\n",
    "docstore = InMemoryDocstore({})\n",
    "index_to_docstore_id = {}  # Empty mapping to start\n",
    "# Read more about the models here:\n",
    "# https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html\n",
    "# https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html\n",
    "\n",
    "# Helper function: generates a text embedding\n",
    "def generate_text_embedding(text):\n",
    "    response = client.invoke_model(\n",
    "        modelId=text_embed_model,\n",
    "        body=json.dumps({\"inputText\": text})\n",
    "    )\n",
    "    #print(\"Response keys:\", response.keys()) #dict_keys(['ResponseMetadata', 'contentType', 'body'])\n",
    "    response_body = json.loads(response['body'].read())    \n",
    "    embedding = response_body['embedding']\n",
    "    return embedding\n",
    "\n",
    "# Helper function: generates an image embedding\n",
    "def generate_image_embedding(image_data):\n",
    "    response = client.invoke_model(\n",
    "        modelId=image_embed_model,\n",
    "        body={\"image\": image_data}\n",
    "    )\n",
    "    response_body = json.loads(response['body'].read())\n",
    "    embedding = response_body['embedding']\n",
    "    return embedding\n",
    "\n",
    "# Helper function: generates a unique ID based on text it's given\n",
    "# You could also try hashlib.sha256 or hashlib.sha3_256 for improved collision (duplicate) resistance\n",
    "def get_unique_id(text):\n",
    "    return hashlib.md5(text.encode()).hexdigest()\n",
    "\n",
    "# Step 1: Load or Initialize FAISS Index\n",
    "if os.path.exists(INDEX_PATH):\n",
    "    print(\"Loading existing FAISS index...\")\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "else:\n",
    "    print(\"No FAISS index found. Initializing a new one...\")\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
    "\n",
    "# Step 2: Add rows to FAISS if they are not already there\n",
    "def add_to_faiss_if_missing(row_data):\n",
    "    text, style, image = row_data[\"text\"], row_data[\"style\"], row_data[\"image\"]\n",
    "    \n",
    "    # Check if already in FAISS\n",
    "    # docstore.search() may not be performant for large-scale applications; Redis may be better...\n",
    "    unique_id = get_unique_id(f\"{text} - Style: {style}\")\n",
    "    if not docstore.search(unique_id):\n",
    "        # Embed each component\n",
    "        text_embedding = generate_text_embedding(text)\n",
    "        style_embedding = generate_text_embedding(style)\n",
    "        #image_embedding = generate_image_embedding(image)\n",
    "    \n",
    "        # Add to FAISS\n",
    "        faiss_index = index.ntotal  # Get the next available index in FAISS\n",
    "        #vector_db.add_texts([unique_id], embeddings=[text_embedding + style_embedding + image_embedding])\n",
    "        vector_db.add_texts([unique_id], embeddings=[text_embedding + style_embedding])\n",
    "        docstore[doc_id] = unique_id\n",
    "\n",
    "# Step 3: Process dataset and add rows to FAISS as necessary\n",
    "vector_db = FAISS(\n",
    "    index=index,\n",
    "    embedding_function=add_to_faiss_if_missing,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id\n",
    ")\n",
    "\n",
    "for row in small_dataset:  # Assuming `dataset` is iterable with each row as a dictionary of `text`, `style`, `image`\n",
    "    add_to_faiss_if_missing(row)\n",
    "\n",
    "# Step 4: Save the FAISS index\n",
    "faiss.write_index(vector_db.index, INDEX_PATH)\n",
    "# TODO: can the docstore also be saved to disk like this?\n",
    "print(\"FAISS index saved to disk.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3477396-cec9-4ae4-97a9-fb355fe142fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see how many embeddings are there:\n",
    "print(f\"Number of embeddings in FAISS index: {index.ntotal}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11970e-6d96-4700-9a1c-54a1db779fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Make sure embedding functions work:\n",
    "first_row = small_dataset[0]\n",
    "print(\"First row data:\", first_row)\n",
    "\n",
    "sample_text = first_row[\"text\"]\n",
    "sample_style = first_row[\"style\"]\n",
    "#sample_image = first_row[\"image\"]  # Ensure this is in the correct format for `generate_image_embedding`\n",
    "\n",
    "# Run embedding functions\n",
    "sample_text_embedding = generate_text_embedding(sample_text)\n",
    "sample_style_embedding = generate_text_embedding(sample_style)\n",
    "#print(\"\\n\\nText embedding:\", sample_text_embedding)\n",
    "#print(\"\\n\\nStyle embedding:\", sample_style_embedding)\n",
    "#print(\"Image embedding:\", generate_image_embedding(sample_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb32eac-2682-496e-9bbd-dd8ac04a3232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've heard Claude is nice in conversation, so let's try it!\n",
    "\n",
    "# Convert embeddings to text for demonstration purposes\n",
    "embedding_text = f\"Text embedding: {sample_text_embedding[:5]}... Style embedding: {sample_style_embedding[:5]}...\"  # Truncate for readability\n",
    "\n",
    "# Formulate a prompt using the embeddings\n",
    "# Define your conversation in the required Messages API format\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"The following are embeddings based on a text and style: {embedding_text}. Translate the original text and style description to French.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# The payload includes the \"messages\" key and other required parameters\n",
    "payload = {\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 200,  # Specifies the maximum tokens for Claude's response\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\"  # Required version field\n",
    "}\n",
    "\n",
    "\n",
    "# Make the request to invoke Claude's model\n",
    "response = client.invoke_model(\n",
    "    modelId=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    body=json.dumps(payload),  # Adjust field as required by Claude's setup\n",
    "    contentType=\"application/json\",\n",
    "    accept=\"application/json\"\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "response_body = json.loads(response[\"body\"].read())\n",
    "#print(\"Response body keys:\", response_body.keys())\n",
    "#Response body keys: dict_keys(['id', 'type', 'role', 'model', 'content', 'stop_reason', 'stop_sequence', 'usage'])\n",
    "\n",
    "# Extract and print the content under the \"content\" key\n",
    "claude_response = response_body.get(\"content\", [{}])[0].get(\"text\", \"No content available.\")\n",
    "print(\"Claude's response:\", claude_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
