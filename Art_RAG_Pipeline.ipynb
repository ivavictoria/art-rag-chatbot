{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8cccd5f-0618-45c9-bf73-e3e02a5b3230",
   "metadata": {},
   "source": [
    "# Iva's Art RAG Pipeline\n",
    "\n",
    "Still a work in progress, but it's coming along!\n",
    "\n",
    "This first cell imports our dependencies and loads the wikiart-subjects dataset.\n",
    "It also creates a small_dataset of the first 5 dataset rows for testing purposes.\n",
    "\n",
    "Everything here is in order that I worked on it. You'll see that I got a little too absorbed with trying to set up a FAISS vector database, that I didn't have time to dig deeper into actually using Claude more deeply.\n",
    "\n",
    "But I DO at least get connected to everything -- I can generate embeddings and get responses from Claude via the AWS client I was provided with.\n",
    "\n",
    "The next step is to finish turning my slapdash tinkering with FAISS into a working vector DB. After that, the ACTUAL fun begins: Taking user input (text queries and/or images), embedding the query, doing a relevance search with FAISS, getting the most relevant wikidata-art entries, and giving those to Claude to use as context while generating its response to the query. There is some loose pseudocode at the end of the notebook describing the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d12b590b-ccfb-475b-9ab2-c152a66ea555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               image  \\\n",
      "0  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
      "1  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
      "2  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
      "3  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
      "4  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...   \n",
      "\n",
      "                                             text               style  \n",
      "0   the cover of the book the magician's daughter  art-nouveau-modern  \n",
      "1               the adoration of the holy trinity             baroque  \n",
      "2  a painting of a man on horseback with two dogs  art-nouveau-modern  \n",
      "3          a painting of a woman in a white dress             baroque  \n",
      "4           a painting of a woman laying on a bed       expressionism  \n"
     ]
    }
   ],
   "source": [
    "# This requires two dependencies: langchain and langchain_aws\n",
    "# You also need datasets to use the wikiart-subject dataset, and datasets requires Pillow to decode images\n",
    "# I've also chosen to use FAISS as a vector store & cache, which requires faiss-cpu and langchain-community\n",
    "# pip install langchain langchain_aws\n",
    "# pip install datasets Pillow\n",
    "# pip install faiss-cpu langchain-community\n",
    "\n",
    "# Available models:\n",
    "# amazon.titan-embed-text-v1\n",
    "# amazon.titan-embed-image-v1\n",
    "# anthropic.claude-3-5-sonnet-20240620-v1:0\n",
    "# cohere.embed-multilingual-v3\n",
    "# meta.llama3-70b-instruct-v1:0\n",
    "\n",
    "import boto3\n",
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "from langchain_aws.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "from datasets import load_dataset\n",
    "import faiss\n",
    "import os\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "# Load the dataset:\n",
    "# https://huggingface.co/datasets/jlbaker361/wikiart-subjects\n",
    "full_dataset = load_dataset(\"jlbaker361/wikiart-subjects\")\n",
    "\n",
    "# For development, let's use a smaller subset of the full dataset, since it's quite large (815MB)\n",
    "# Let's take a 5% random sample from the \"train\" split.\n",
    "#small_dataset = full_dataset[\"train\"].train_test_split(test_size=0.05)[\"test\"]\n",
    "\n",
    "# Optionally, if we set also a seed, we'll get the same subset each time; the consistency can be handy for testing & debugging.\n",
    "small_dataset = full_dataset[\"train\"].train_test_split(test_size=0.05, seed=42)[\"test\"]\n",
    "\n",
    "# Just to see that the data is there, convert it to DataFrame and display the first few rows\n",
    "small_dataset_df = small_dataset.to_pandas()\n",
    "print(small_dataset_df.head())  # Display the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b6a3d1-6737-492b-9ebe-e955843c05ee",
   "metadata": {},
   "source": [
    "## Connect to AWS and start a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d302793-f5f4-4f13-9d67-e0a43525179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a session with AWS via the Boto3 Python SDK.\n",
    "session = boto3.Session(\n",
    "  aws_access_key_id='[AWS_ACCESS_KEY_ID]',\n",
    "  aws_secret_access_key='[AWS_SECRET_ACCESS_KEY]',\n",
    "  region_name='us-east-1'\n",
    ")\n",
    "\n",
    "# Connect to Bedrock services so we can access models for embeddings & chat.\n",
    "client = session.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c3ace4-6196-44bc-a0e9-c3d7a2cbe0e7",
   "metadata": {},
   "source": [
    "## WIP Creating embeddings and storing them in a vector database\n",
    "\n",
    "Now we need to create embeddings for our dataset and store them in a vector database.\n",
    "I have chosen FAISS as a vector db because it runs locally instead of in the cloud, so that I don't have to sign up for an account, a subscription, etc.\n",
    "\n",
    "For added efficiency, I'm also attempting to store the FAISS db in a local index file, to act as a cache so that I don't recreate embeds over and over if I restart Jupyter, preventing wasteful and costly API calls to AWS.\n",
    "\n",
    "Just for my own reference, I've saved links where I can read more about the titan models:\n",
    "- https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html\n",
    "- https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07f4a9dc-2cb8-463e-b549-93f558f6df5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing FAISS index...\n",
      "FAISS index saved to disk.\n"
     ]
    }
   ],
   "source": [
    "text_embed_model = 'amazon.titan-embed-text-v1'\n",
    "image_embed_model = 'amazon.titan-embed-image-v1'\n",
    "INDEX_PATH = \"faiss_index_file\"  # File path for saving/loading FAISS index\n",
    "dimension = 1024  # Default vector dimensions for Amazon's titan embed models; 384 and 256 also supported\n",
    "docstore = InMemoryDocstore({})\n",
    "index_to_docstore_id = {}  # Empty mapping to start\n",
    "\n",
    "# Helper function: generates a text embedding\n",
    "def generate_text_embedding(text):\n",
    "    response = client.invoke_model(\n",
    "        modelId=text_embed_model,\n",
    "        body=json.dumps({\"inputText\": text})\n",
    "    )\n",
    "    #print(\"Response keys:\", response.keys()) #dict_keys(['ResponseMetadata', 'contentType', 'body'])\n",
    "    response_body = json.loads(response['body'].read())    \n",
    "    embedding = response_body['embedding']\n",
    "    return embedding\n",
    "\n",
    "# Helper function: generates an image embedding\n",
    "def generate_image_embedding(image_data):\n",
    "    response = client.invoke_model(\n",
    "        modelId=image_embed_model,\n",
    "        body={\"image\": image_data}\n",
    "    )\n",
    "    response_body = json.loads(response['body'].read())\n",
    "    embedding = response_body['embedding']\n",
    "    return embedding\n",
    "\n",
    "# Helper function: generates a unique ID based on text it's given\n",
    "# You could also try hashlib.sha256 or hashlib.sha3_256 for improved collision (duplicate) resistance\n",
    "def get_unique_id(text):\n",
    "    return hashlib.md5(text.encode()).hexdigest()\n",
    "\n",
    "# Step 1: Load or Initialize FAISS Index\n",
    "if os.path.exists(INDEX_PATH):\n",
    "    print(\"Loading existing FAISS index...\")\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "else:\n",
    "    print(\"No FAISS index found. Initializing a new one...\")\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
    "\n",
    "# Step 2: Add rows to FAISS if they are not already there\n",
    "def add_to_faiss_if_missing(row_data):\n",
    "    text, style, image = row_data[\"text\"], row_data[\"style\"], row_data[\"image\"]\n",
    "    \n",
    "    # Check if already in FAISS\n",
    "    # docstore.search() may not be performant for large-scale applications; Redis may be better...\n",
    "    unique_id = get_unique_id(f\"{text} - Style: {style}\")\n",
    "    if not docstore.search(unique_id):\n",
    "        # Embed each component\n",
    "        text_embedding = generate_text_embedding(text)\n",
    "        style_embedding = generate_text_embedding(style)\n",
    "        #image_embedding = generate_image_embedding(image)\n",
    "    \n",
    "        # Add to FAISS\n",
    "        faiss_index = index.ntotal  # Get the next available index in FAISS\n",
    "        #vector_db.add_texts([unique_id], embeddings=[text_embedding + style_embedding + image_embedding])\n",
    "        vector_db.add_texts([unique_id], embeddings=[text_embedding + style_embedding])\n",
    "        docstore[doc_id] = unique_id\n",
    "\n",
    "# Step 3: Process dataset and add rows to FAISS as necessary\n",
    "vector_db = FAISS(\n",
    "    index=index,\n",
    "    embedding_function=add_to_faiss_if_missing,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id\n",
    ")\n",
    "\n",
    "for row in small_dataset:  # Assuming `dataset` is iterable with each row as a dictionary of `text`, `style`, `image`\n",
    "    add_to_faiss_if_missing(row)\n",
    "\n",
    "# Step 4: Save the FAISS index\n",
    "faiss.write_index(vector_db.index, INDEX_PATH)\n",
    "# TODO: can the docstore also be saved to disk like this?\n",
    "print(\"FAISS index saved to disk.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3477396-cec9-4ae4-97a9-fb355fe142fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings in FAISS index: 0\n"
     ]
    }
   ],
   "source": [
    "# Test to see how many embeddings are there:\n",
    "print(f\"Number of embeddings in FAISS index: {index.ntotal}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cb20f5-54b3-4694-8423-1a44fcd18769",
   "metadata": {},
   "source": [
    "## Sending a single message to Claude for demo purposes\n",
    "\n",
    "Since I'm still working on the above embedding workflow with a FAISS vector DB, here is at least a working call that sends a query to Claude and receives a response.\n",
    "\n",
    "I chose \"translate the text to French\" after seeing it in an example in documentation. An interesting 'Hello World' of sorts.\n",
    "\n",
    "Naturally, Claude will kindly explain that it cannot translate vectors into French, and it needs the text.\n",
    "I was too focused on trying to send SOMETHING to Claude that I forgot I don't need to send it the _vectors themselves_ at all... ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb32eac-2682-496e-9bbd-dd8ac04a3232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row data: {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=187x500 at 0x158D002C0>, 'text': \"the cover of the book the magician's daughter\", 'style': 'art-nouveau-modern'}\n",
      "Claude's response: I apologize, but I'm not able to directly translate the original text and style description to French based solely on the embeddings provided. Embeddings are numerical representations of text that capture semantic meaning, but they don't contain the actual words or content of the original text.\n",
      "\n",
      "To translate the original text and style description to French, I would need access to the actual text and style description in their original language (presumably English). Embeddings alone do not contain enough information to reconstruct the original text or perform a translation.\n",
      "\n",
      "If you have the original text and style description available, I'd be happy to assist with translating those to French. Otherwise, I can only provide general information about embeddings and their use in natural language processing, but cannot perform a translation based solely on the embedding vectors.\n"
     ]
    }
   ],
   "source": [
    "# I've heard Claude is nice in conversation, so let's try it!\n",
    "\n",
    "# Test: Make sure embedding functions work:\n",
    "first_row = small_dataset[0]\n",
    "print(\"First row data:\", first_row)\n",
    "\n",
    "sample_text = first_row[\"text\"]\n",
    "sample_style = first_row[\"style\"]\n",
    "#sample_image = first_row[\"image\"]  # Ensure this is in the correct format for `generate_image_embedding`\n",
    "\n",
    "# Run embedding functions\n",
    "sample_text_embedding = generate_text_embedding(sample_text)\n",
    "sample_style_embedding = generate_text_embedding(sample_style)\n",
    "#print(\"\\n\\nText embedding:\", sample_text_embedding)\n",
    "#print(\"\\n\\nStyle embedding:\", sample_style_embedding)\n",
    "#print(\"Image embedding:\", generate_image_embedding(sample_image))\n",
    "\n",
    "# Convert embeddings to text for demonstration purposes\n",
    "embedding_text = f\"Text embedding: {sample_text_embedding[:5]}... Style embedding: {sample_style_embedding[:5]}...\"  # Truncate for readability\n",
    "\n",
    "# Formulate a prompt using the embeddings\n",
    "# Define your conversation in the required Messages API format\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"The following are embeddings based on a text and style: {embedding_text}. Translate the original text and style description to French.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# The payload includes the \"messages\" key and other required parameters\n",
    "payload = {\n",
    "    \"messages\": messages,\n",
    "    \"max_tokens\": 200,  # Specifies the maximum tokens for Claude's response\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\"  # Required version field\n",
    "}\n",
    "\n",
    "\n",
    "# Make the request to invoke Claude's model\n",
    "response = client.invoke_model(\n",
    "    modelId=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    body=json.dumps(payload),  # Adjust field as required by Claude's setup\n",
    "    contentType=\"application/json\",\n",
    "    accept=\"application/json\"\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "response_body = json.loads(response[\"body\"].read())\n",
    "#print(\"Response body keys:\", response_body.keys())\n",
    "#Response body keys: dict_keys(['id', 'type', 'role', 'model', 'content', 'stop_reason', 'stop_sequence', 'usage'])\n",
    "\n",
    "# Extract and print the content under the \"content\" key\n",
    "claude_response = response_body.get(\"content\", [{}])[0].get(\"text\", \"No content available.\")\n",
    "print(\"Claude's response:\", claude_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19f1c8d-114c-491d-830d-577242629c81",
   "metadata": {},
   "source": [
    "## How the RAG process should work eventually\n",
    "\n",
    "PSEUDOCODE for how my RAG process should work in a nutshell, when it all comes together.\n",
    "\n",
    "After I connected to Claude I realized: At no point should I actually need to send the embeddings to Claude.\n",
    "Instead, I should be using my own vector DB to decide which of my embeddings are most relevant.\n",
    "I should embed each query, do a similarity search on the DB, gather the most relevant data, and then send the text to Claude for it to use as context in its response.\n",
    "\n",
    "I had set the image embeddings aside to just focus on the text first, trying to take baby steps, since all of these tools are new to me.\n",
    "But the image embeddings will be interesting down the line as well, because it can compare the images in the DB to one another, or to a human-provided image (kind of like Google's reverse image search).\n",
    "\n",
    "```python\n",
    "# # Step 1: Initialize embedding and chat models\n",
    "embedder = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\")\n",
    "chat_model = ChatBedrock(model_id=\"claude-v1\")\n",
    "\n",
    "# Step 2: Generate and store embeddings for your dataset (only done once)\n",
    "document_texts = [\"text of document 1\", \"text of document 2\", ...]  # Replace with actual documents\n",
    "document_embeddings = [embedder.embed_query(doc) for doc in document_texts]\n",
    "\n",
    "# Step 3: When a query is received, embed it and find relevant documents\n",
    "query = \"What is the summary of X topic?\"\n",
    "query_embedding = embedder.embed_query(query)\n",
    "\n",
    "# Find top matches in FAISS or another similarity tool (pseudo-code)\n",
    "relevant_doc_ids = faiss_index.search(query_embedding, top_k=5)  # `top_k` is number of relevant docs to retrieve\n",
    "\n",
    "# Step 4: Retrieve the relevant texts and construct the prompt\n",
    "relevant_texts = [document_texts[i] for i in relevant_doc_ids]\n",
    "context = \"\\n\\n\".join([f\"Passage {i+1}: {text}\" for i, text in enumerate(relevant_texts)])\n",
    "prompt = f\"{context}\\n\\nQuestion: {query}\"\n",
    "\n",
    "# Step 5: Send prompt to Claude for a generated response\n",
    "response = chat_model(prompt)\n",
    "print(\"Claude's RAG-based response:\", response.content)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
